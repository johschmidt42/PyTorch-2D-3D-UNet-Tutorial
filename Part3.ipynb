{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Training with customdataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.transform import resize\n",
    "from customdatasets import SegmentationDataSet1\n",
    "from transformations import (\n",
    "    ComposeDouble,\n",
    "    AlbuSeg2d,\n",
    "    FunctionWrapperDouble,\n",
    "    normalize_01,\n",
    "    create_dense_target,\n",
    ")\n",
    "from unet import UNet\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "# root directory\n",
    "root = pathlib.Path.cwd() / \"Carvana\"\n",
    "\n",
    "\n",
    "def get_filenames_of_path(path: pathlib.Path, ext: str = \"*\"):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# input and target files\n",
    "inputs = get_filenames_of_path(root / \"Input\")\n",
    "targets = get_filenames_of_path(root / \"Target\")\n",
    "\n",
    "# training transformations and augmentations\n",
    "transforms_training = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "        AlbuSeg2d(albumentations.HorizontalFlip(p=0.5)),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# validation transformations\n",
    "transforms_validation = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# random seed\n",
    "random_seed = 42\n",
    "\n",
    "# split dataset into training set and validation set\n",
    "train_size = 0.8  # 80:20 split\n",
    "\n",
    "inputs_train, inputs_valid = train_test_split(\n",
    "    inputs, random_state=random_seed, train_size=train_size, shuffle=True\n",
    ")\n",
    "\n",
    "targets_train, targets_valid = train_test_split(\n",
    "    targets, random_state=random_seed, train_size=train_size, shuffle=True\n",
    ")\n",
    "\n",
    "# inputs_train, inputs_valid = inputs[:80], inputs[80:]\n",
    "# targets_train, targets_valid = targets[:80], targets[:80]\n",
    "\n",
    "# dataset training\n",
    "dataset_train = SegmentationDataSet1(\n",
    "    inputs=inputs_train, targets=targets_train, transform=transforms_training\n",
    ")\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet1(\n",
    "    inputs=inputs_valid, targets=targets_valid, transform=transforms_validation\n",
    ")\n",
    "\n",
    "# dataloader training\n",
    "dataloader_training = DataLoader(dataset=dataset_train, batch_size=2, shuffle=True)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DatasetViewer instances\n",
    "from visual import DatasetViewer\n",
    "\n",
    "dataset_viewer_training = DatasetViewer(dataset_train)\n",
    "dataset_viewer_validation = DatasetViewer(dataset_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for training dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_training.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for validation dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_validation.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    n_blocks=4,\n",
    "    start_filters=32,\n",
    "    activation=\"relu\",\n",
    "    normalization=\"batch\",\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    ").to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    training_dataloader=dataloader_training,\n",
    "    validation_dataloader=dataloader_validation,\n",
    "    lr_scheduler=None,\n",
    "    epochs=2,\n",
    "    epoch=0,\n",
    "    notebook=True,\n",
    ")\n",
    "\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with customdataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pathlib\n",
    "from transformations import (\n",
    "    ComposeDouble,\n",
    "    normalize_01,\n",
    "    AlbuSeg2d,\n",
    "    FunctionWrapperDouble,\n",
    "    create_dense_target,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from customdatasets import SegmentationDataSet2\n",
    "import torch\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from trainer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.transform import resize\n",
    "import albumentations\n",
    "\n",
    "# root directory\n",
    "root = pathlib.Path.cwd() / \"Carvana\"\n",
    "\n",
    "\n",
    "def get_filenames_of_path(path: pathlib.Path, ext: str = \"*\"):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# input and target files\n",
    "inputs = get_filenames_of_path(root / \"Input\")\n",
    "targets = get_filenames_of_path(root / \"Target\")\n",
    "\n",
    "# pre-transformations\n",
    "pre_transforms = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# training transformations and augmentations\n",
    "transforms_training = ComposeDouble(\n",
    "    [\n",
    "        AlbuSeg2d(albumentations.HorizontalFlip(p=0.5)),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# validation transformations\n",
    "transforms_validation = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# random seed\n",
    "random_seed = 42\n",
    "\n",
    "# split dataset into training set and validation set\n",
    "train_size = 0.8  # 80:20 split\n",
    "\n",
    "inputs_train, inputs_valid = train_test_split(\n",
    "    inputs, random_state=random_seed, train_size=train_size, shuffle=True\n",
    ")\n",
    "\n",
    "targets_train, targets_valid = train_test_split(\n",
    "    targets, random_state=random_seed, train_size=train_size, shuffle=True\n",
    ")\n",
    "\n",
    "# inputs_train, inputs_valid = inputs[:80], inputs[80:]\n",
    "# targets_train, targets_valid = targets[:80], targets[:80]\n",
    "\n",
    "# dataset training\n",
    "dataset_train = SegmentationDataSet2(\n",
    "    inputs=inputs_train,\n",
    "    targets=targets_train,\n",
    "    transform=transforms_training,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet2(\n",
    "    inputs=inputs_valid,\n",
    "    targets=targets_valid,\n",
    "    transform=transforms_validation,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n",
    "\n",
    "# dataloader training\n",
    "dataloader_training = DataLoader(dataset=dataset_train, batch_size=2, shuffle=True)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DatasetViewer instances\n",
    "from visual import DatasetViewer\n",
    "\n",
    "dataset_viewer_training = DatasetViewer(dataset_train)\n",
    "dataset_viewer_validation = DatasetViewer(dataset_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for training dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_training.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for validation dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_validation.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    n_blocks=4,\n",
    "    start_filters=32,\n",
    "    activation=\"relu\",\n",
    "    normalization=\"batch\",\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    ").to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    training_dataloader=dataloader_training,\n",
    "    validation_dataloader=dataloader_validation,\n",
    "    lr_scheduler=None,\n",
    "    epochs=10,\n",
    "    epoch=0,\n",
    "    notebook=True,\n",
    ")\n",
    "\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with customdataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pathlib\n",
    "from transformations import (\n",
    "    ComposeDouble,\n",
    "    normalize_01,\n",
    "    AlbuSeg2d,\n",
    "    FunctionWrapperDouble,\n",
    "    create_dense_target,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from customdatasets import SegmentationDataSet3\n",
    "import torch\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from trainer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.transform import resize\n",
    "import albumentations\n",
    "\n",
    "# root directory\n",
    "root = pathlib.Path.cwd() / \"Carvana\"\n",
    "\n",
    "\n",
    "def get_filenames_of_path(path: pathlib.Path, ext: str = \"*\"):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# input and target files\n",
    "inputs = get_filenames_of_path(root / \"Input\")\n",
    "targets = get_filenames_of_path(root / \"Target\")\n",
    "\n",
    "# pre-transformations\n",
    "pre_transforms = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# training transformations and augmentations\n",
    "transforms_training = ComposeDouble(\n",
    "    [\n",
    "        AlbuSeg2d(albumentations.HorizontalFlip(p=0.5)),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# validation transformations\n",
    "transforms_validation = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# random seed\n",
    "random_seed = 42\n",
    "\n",
    "# split dataset into training set and validation set\n",
    "train_size = 0.8  # 80:20 split\n",
    "\n",
    "inputs_train, inputs_valid = train_test_split(\n",
    "    inputs, random_state=random_seed, train_size=train_size, shuffle=True\n",
    ")\n",
    "\n",
    "targets_train, targets_valid = train_test_split(\n",
    "    targets, random_state=random_seed, train_size=train_size, shuffle=True\n",
    ")\n",
    "\n",
    "# inputs_train, inputs_valid = inputs[:80], inputs[80:]\n",
    "# targets_train, targets_valid = targets[:80], targets[:80]\n",
    "\n",
    "# dataset training\n",
    "dataset_train = SegmentationDataSet3(\n",
    "    inputs=inputs_train,\n",
    "    targets=targets_train,\n",
    "    transform=transforms_training,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet3(\n",
    "    inputs=inputs_valid,\n",
    "    targets=targets_valid,\n",
    "    transform=transforms_validation,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n",
    "\n",
    "# dataloader training\n",
    "dataloader_training = DataLoader(dataset=dataset_train, batch_size=2, shuffle=True)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    n_blocks=4,\n",
    "    start_filters=32,\n",
    "    activation=\"relu\",\n",
    "    normalization=\"batch\",\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    ").to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    training_dataloader=dataloader_training,\n",
    "    validation_dataloader=dataloader_validation,\n",
    "    lr_scheduler=None,\n",
    "    epochs=10,\n",
    "    epoch=0,\n",
    "    notebook=True,\n",
    ")\n",
    "\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_name = \"carvana_model.pt\"\n",
    "torch.save(model.state_dict(), pathlib.Path.cwd() / model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    n_blocks=4,\n",
    "    start_filters=32,\n",
    "    activation=\"relu\",\n",
    "    normalization=\"batch\",\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    ").to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lr_rate_finder import LearningRateFinder\n",
    "\n",
    "lrf = LearningRateFinder(model, criterion, optimizer, device)\n",
    "lrf.fit(dataloader_training, steps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import plot_training\n",
    "\n",
    "fig = plot_training(\n",
    "    training_losses,\n",
    "    validation_losses,\n",
    "    lr_rates,\n",
    "    gaussian=True,\n",
    "    sigma=1,\n",
    "    figsize=(10, 4),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
